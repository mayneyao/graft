---
title: 0001 - Direct Storage Architecture
rfc:
  id: 0001
  slug: direct-storage-architecture
  startDate: 2025-05-14
  issue: ~
  pr: ~
---

import RfcInfo from "@components/RfcInfo.astro";

<RfcInfo {...frontmatter.rfc} />

Build a new Graft client library (called `graft-kernel`) which directly interfaces with object storage, eliminating the need for the MetaStore and PageStore, and setting Graft up as a viable replacement to systems like Litestream. Graft should focus on providing best-in-class PITR, branching, and sparse replication for page-based workloads.

## Motivation

An overview of why we should consider making this major change to Graft's architecture.

### Existing state of Graft

Currently Graft requires a MetaStore and PageStore to support replication to and from object storage. This architecture has the following advantages and disadvantages:

**Advantages**

- The MetaStore can efficiently rollup commits to fast forward clients, increasing performance and enabling instant read replicas
- The PageStore acts as a smart cache, allowing clients to pull only the pages they need at the edge.
- The PageStore is able to collocate writes to multiple Volumes in the same Segment which can reduce the cost and overhead of small transactions.

**Disadvantages**

- There is little isolation between data in different Volumes. Graft will need to roll out a comprehensive encryption + authorization layer to work for production workloads. This is a huge cost in terms of testing and engineering.
- Users must run two services to take full advantage of Graft, this makes Graft much harder to use.

### [Datasette]

In a discussion with [Simon Willison] and [Alex Garcia], we talked about some of their dream features for SQLite + Graft:

**Rollback database to earlier version**
The ability to cheaply rollback a database would make risky features like giving an LLM read/write access to your database much safer. Additionally, the ability to branch a database at a particular version may enable risk-free experimentation and testing.

**Read-only replication**
Cheap and fast read-only replication to horizontally scale a heavy query workload over multiple machines, or simply to expose data to less-trusted users.

**Composability with IAM permissions**
Currently, Datasette uses IAM keys limited to a single S3 prefix to restrict Litestream's access to a single tenant's data. This ensures that a bug in Litestream can affect at most a single tenant.

This feature implies that data does not cross "tenant" boundaries (or in this case, the configured S3 prefix).

[Datasette]: https://www.datasette.cloud/
[Simon Willison]: https://simonwillison.net/
[Alex Garcia]: https://alexgarcia.xyz/

### Object storage scalability

In a discussion with a potential user, they expressed reservations due to the layer of indirection between the Graft client and object storage. Their main argument is that S3 is already proven to handle extremely high scale. They would be more comfortable using Graft if clients connected directly to object storage to pull changes. In some cases, this may also reduce costs due to free bandwidth between compute and S3.

### New user experience

Graft should work out of the box without needing additional services to run. By supporting direct access to object storage, it will be easier to get started and embed Graft in an application.

## Guide-level Explanation

A high level explanation of how this feature works, and would change the behavior of existing Graft clients such as `graft-sqlite` and `graft-fuse`.

### `graft-kernel`

`graft-kernel` implements the Graft Kernel which supersedes the functionality of the `graft-client` and `graft-server` crates. The Kernel provides access to a remote Volume Catalog, local storage, and client functionality to downstream crates like `graft-sqlite` and `graft-fuse`.

**Client functionality**
Clients such as `graft-sqlite` and `graft-fuse` will use the `graft-kernel` to operate on Volumes. The Kernel is designed to be embedded in the application, performing I/O in a small set of background threads. The Kernel will be implemented as an async core wrapped with an async and sync API.

### `graft-proxy`

The Graft Proxy is a stateless edge service and caching layer which makes it easier for Graft to replicate to & from devices. Initially it will simply act as a pass-through proxy to object storage with its own authentication and authorization layer.

Graft Proxy exposes a simple API to consumers, enabling two key performance features:

1. Graft Proxy caches reads from object storage.
2. Replica fast-forwarding: collapse a pull graft request and only return the latest segments.

Eventually Graft Proxy will enhance Graft with these features:

- volume subscriptions -> eliminate the need to poll for changes
- granular authorization
- direct byte range puts and gets against Volumes, enabling "dumb clients".

## Reference-level Explanation

Detailed technical breakdown. Cover APIs, algorithms, data structures, formats, edge cases, and performance implications.

### Glossary

- **`Volume Handle`**: A reference to a local-remote Volume pair. Also responsible for tracking synchronization between a local and remote Volume. Documented more in the [Volume Handle](#volume-handle) section.
- **`vid`**: A 16 byte Volume ID using [`GID`] encoding.
- **`sid`**: A 16 byte Segment ID using [`GID`] encoding.
- **`Snapshot`**: A frozen point-in-time view of a Volume.
- **`lsn`**: Documented in the [Volume log](#volume-log) section.
- **`pageidx`**: A 4 byte page index, representing the index of a page within a Volume. Valid range: `[1, 2^32)`.
- **`Splinter`**: A compressed bitset, used to keep track of which `pageidxs` appear in a Segment.
- **`Segment`**: A sequence of pages, sorted by `pageidx`.
- **`VolumeRef`**: A `(vid, lsn)` tuple, representing a fixed point in a Volumes history.
- **`CommitHash`**: Documented in the [Commit hash](#commit-hash) section.

### Volume log

A volume’s durable state consists of a **Checkpoint** and a **Log**.

- **Checkpoint** — a point-in-time mapping from each non-empty `pageidx` to the version (by LSN) that was current when the checkpoint was taken.
- **Log** — an append-only sequence of log records. Each record contains the set of pages modified since the previous LSN.

#### Log Sequence Numbers (LSNs)

| Property     | Definition                                                         |
| ------------ | ------------------------------------------------------------------ |
| **Domain**   | Unsigned 64-bit integer in the range `[1, 2^64)`. Zero is invalid. |
| **Ordering** | Strictly increasing, gap-free, and scoped per volume.              |
| **Encoding** | Canonical representation is `CBE64`: one’s-complement, big-endian. |

Because `0` is never a valid LSN, it's available as a sentinel value.

#### `CBE64` Encoding

`CBE64` stands for Ones-**C**ompliment **B**ig **E**dian.

- **Binary form** — 8-byte array. Bytewise comparison yields descending numeric order. Used for space-efficient storage, such as in embedded key-value stores like [Fjall].
- **Hex form** — 16-character, zero-padded, uppercase hexadecimal string. Lexicographically sorts in the same order as the binary form. Used where human readability is preferred, such as object store keys.

The CBE64 encoding allows both local key-value stores and object stores to perform forward iteration over keys to process log records in descending LSN order, without additional index structures.

### Commit hash

To verify data integrity, we attach a hash to each `Commit`. To compute the hash, we use the following method:

First, a [blake3] hash is computed for each Segment:

```python
def segment_hash(pages):
  hasher = blake3::new()
  hasher.write(HashHeader::Segment(PAGE_SIZE));
  # pages must be in order by pageidx
  for page in pages:
    hasher.write(page)
  return hasher.hash()
```

Then a Commit hash is produced by combining the Segment hashes together:

```python
def commit_hash(snapshot, segment_hashes):
  hasher = blake3::new()
  hasher.write(HashHeader::Commit(snapshot));
  # segment_hashes must be in order by pageidx
  for hash in segment_hashes:
    hasher.write(hash)
  return hasher.hash()
```

This two phase approach allows the Segment hashes to be produced in parallel, before combining them into a Commit hash.

Note that the Commit's snapshot is passed in. This ensures the Hash's uniqueness incorporates the Volume ID, LSN, and page count.

### Volume Handle

Rather than forcing users to reference Volumes by ID, Graft exposes Volume Handles. Volume Handles are pointers to a local and remote Volume. Volume Handles only exist on a single client, and are not shared between clients or pushed to a remote.

Each Volume Handle has a name given to it at creation time. The name must match the regex `^[_a-z][-_a-z0-9]{0,127}$` (alphanumeric, starts with number or underscore, max len 128 chars) and be unique on the client.

A Volume Handle always has a local-only Volume associated with it. This Volume is used for all local reads and writes.

A Volume Handle may be linked to a remote Volume. In this case, the local and remote Volumes will be kept in sync by the sync subsystem.

### Remote storage

#### Object storage keyspace

Graft will store all of a Volume's data in Object Storage using the following keyspace layout:

```
{prefix} /
  {vid} /
    control: Control
    forks /
      {fork-vid}: Fork
    checkpoints: CheckpointSet
    log /
      {lsn}: Commit
    segments /
      {sid}: Segment
```

This flexible layout allows users to isolate tenants from one another by simply providing a unique `{prefix}`. This can be helpful when using AWS IAM to scope access keys to particular S3 prefixes for example.

#### Object storage schemas

All of the files aside from Segments are encoded using Protobuf. Serialized Protobuf is wrapped with a zerocopy `ProtobufEnvelope`:

```rust
enum ProtobufMessage {
    GraftRemoteV1Control = 1,
    GraftRemoteV1Fork = 2,
    GraftRemoteV1CheckpointSet = 3,
    GraftRemoteV1Commit = 4,
}

struct ProtobufEnvelope {
    magic: [u8; 4],
    _padding: [u8; 3],
    message: ProtobufMessage,
    data: [u8]
}
```

Segments are simply encoded as a set of pages stored back-to-back, ordered by `pageidx` ascending. As they are accessed via Byte-Range requests, pages must start at offset 0. Any metadata about a Segment is contained in the Commit log.

```proto
syntax = "proto3";
package graft.remote.v1;

import "google/protobuf/timestamp.proto";

// A Volume has a top level control file stored at
// `{prefix}/volumes/{vid}/control`
// Control files are immutable.
message Control {
  // The Volume ID stored as a 16 byte GID.
  bytes vid = 1;

  // The parent reference if this Volume is a fork.
  optional VolumeRef parent = 2;

  // The creation timestamp of this Volume.
  google.protobuf.Timestamp created_at = 3;
}

// When a Volume is forked, a ref is first written to the parent Volume:
// `{prefix}/volumes/{parent-vid}/forks/{fork-vid}`
// Forks are immutable.
message Fork {
  // The VID of the fork.
  bytes fork_vid = 1;

  // The fork point. Must match the parent field in the Fork's Control file.
  VolumeRef parent = 2;
}

// A reference to a Volume at a particular LSN.
message VolumeRef {
  // The Volume ID stored as a 16 byte GID.
  bytes vid = 1;

  // The referenced LSN.
  uint64 lsn = 2;
}

// A snapshot of a Volume.
message Snapshot {
  // The Volume ID stored as a 16 byte GID.
  bytes vid = 1;

  // The LSN of the Volume at this Snapshot.
  uint64 lsn = 2;

  // The Volume's page count at this Snapshot.
  uint32 page_count = 3;
}

// A Volume's CheckpointSet is stored at `{prefix}/volumes/{vid}/checkpoints`.
// CheckpointSets are updated by the checkpointer via compare-and-swap.
message CheckpointSet {
  // The Volume ID stored as a 16 byte GID.
  bytes vid = 1;

  // The list of checkpoint LSNs.
  repeated uint64 lsns = 2;
}

// Commits are stored at `{prefix}/volumes/{vid}/log/{lsn}`.
// A commit may not include any SegmentRefs if only the Volume's page count has
// changed. This happens when the Volume is extended or truncated without
// additional writes.
// Commits are immutable.
message Commit {
  // The Volume Snapshot at this Commit.
  Snapshot snapshot = 1;

  // A 256 bit CommitHash of this Commit.
  bytes hash = 2;

  // The list of SegmentRefs representing this Commit.
  repeated SegmentRef segments = 3;
}

message SegmentRef {
  // The 16 byte Segment ID.
  bytes sid = 1;

  // The set of pageidxs stored in this Segment.
  // Serialized using Splinter encoding.
  bytes splinter = 2;
}
```

### Local storage

This section documents how clients store data.

#### Local keyspace

Local storage uses [Fjall], a partitioned k/v store. In the following keyspace, the top level keys are independent partitions. The remainder of the keys and the values are encoded using types in the following section.

```
handles / {name} -> VolumeHandle

volumes / {vid} /
  // The Volume's Control, includes it's parent reference
  control -> Control

  // the latest LocalCheckpointSet for this Volume
  checkpoints -> LocalCheckpointSet

log / {vid} / {lsn} -> Snapshot

segments / {vid} / {lsn} / {min_pageidx, max_pageidx} / {sid} -> Splinter

pages / {sid} / {pageidx} -> Page
```

#### Local schemas

Keys which are stored in the local keyspace are encoded using `zerocopy` types. `lsn` values are stored using [`CBE64`](#cbe64-encoding), which ensures they naturally sort in descending order. This allows us to use a forward iterator to quickly find the most recent LSN, which is much more efficient in most k/v stores (including [Fjall]).

The `handles` partition is unique in that it is keyed directly by the `VolumeHandle`'s name rather than one of the following zerocopy types.

```rust
enum VolumeProperty {
    Control = 1,
    Checkpoints = 2,
}

struct VolumeKey {
    vid: VolumeId,
    property: VolumeProperty,
}

struct LogKey {
    vid: VolumeId,
    lsn: CBE64,
}

struct SegmentKey {
    commit: LogKey,
    min_pageidx: PageIdx,
    max_pageidx: PageIdx,
    sid: SegmentId,
}

struct PageKey {
    sid: SegmentId,
    pageidx: PageIdx,
}
```

Values stored locally are encoded using protobuf, using a combination of the remote storage schema and the following additional message types. No envelope is needed for local values as the upgrade process can migrate local data.

```proto
syntax = "proto3";
package graft.local.v1;
import "graft/remote/v1/index.proto";

message LocalCheckpointSet {
  // The etag from the last time we pulled the CheckpointSet, used to only pull
  // changed CheckpointSets
  bytes etag = 1;

  // The list of checkpoint LSNs.
  repeated uint64 lsns = 2;
}

message VolumeHandle {
  // The name of the Volume Handle
  string name = 1;

  // References to the local and remote Volumes, along with LSNs representing their latest successful synchronization.
  graft.remote.v1.VolumeRef local = 2;
  optional graft.remote.v1.VolumeRef remote = 3;

  // Presence of the pending_commit field means that the Push operation is in the process of committing to the remote. If no such Push job is currently running (i.e. it was interrupted), this field must be used to resume or abort the commit process.
  optional PendingCommit pending_commit = 4;
}

message PendingCommit {
  // The resulting remote LSN that the push job is attempting to create
  uint64 remote_lsn = 1;

  // The associated 256 bit blake3 commit hash. This is used to determine
  // whether or not the commit has landed in the remote, in the case that we are
  // interrupted while attempting to push.
  bytes commit_hash = 2;
}
```

### Algorithms

This section details the various key algorithms powering Graft's new direct storage architecture.

#### Locks

The new direct storage architecture uses sharded locks to minimize contention. We need to take extreme care whenever we hold more than one lock in the same thread to ensure that we can't deadlock due to lock ordering.

##### **`handle_lock(name)`**

This sharded lock is held while modifying a Volume Handle. This happens during the following operations.

- **Pull Volume**: After updating the remote and local volumes we complete the pull operation by updating the Volume Handle while holding the handle_lock.
- **Push Volume**: We hold the handle_lock to record the pending commit, as well as later to complete the push and land the commit in the remote volume.

##### **`volume_lock(vid)`**

The sharded volume lock must be held while modifying a Volume's properties in the `volumes` partition. Properties include `checkpoints` and `control`.

##### **`commit_lock(vid)`**

When committing to a Volume, we hold the `commit_lock` to verify the latest snapshot and write out a new snapshot.

#### Volume Reader

Reading from a Volume requires creating a `VolumeReader` from a `VolumeHandle` at either the latest or a specific snapshot.

```python
class VolumeHandle:
  # inherits properties from graft.local.v1.VolumeHandle:
  # name, local, remote, pending_commit

  def reader():
    snapshot = last(f"log/{self.local.vid}")
    return VolumeReader::new(snapshot)

  def reader_at(snapshot):
    assert_eq(snapshot.vid, self.local.vid)
    return VolumeReader::new(snapshot)

def visibility_path(snapshot):
  cursor = VolumeRef { vid: snapshot.vid, lsn: snapshot.lsn }
  path = []
  while cursor:
    if checkpoints = read(f"volumes/{cursor.vid}/checkpoints"):
      if checkpoint = checkpoints.for(snapshot.lsn):
        # found checkpoint, we can terminate the path here
        path.push((cursor.vid, (cursor.lsn)..=(checkpoint.lsn)))
        return path

    # no checkpoint, so scan to the beginning
    path.push((cursor.vid, (cursor.lsn)..=1))
    # and iterate to the parent
    cursor = read(f"volumes/{cursor.vid}/control").parent

  return path

class VolumeReader:
  def new(snapshot):
    self.snapshot = snapshot
    self.path = visibility_path(snapshot)

  def read_page(self, pageidx):
    if not self.snapshot.page_count.contains(pageidx):
      return None
    for key, splinter in iter_segments(self.path):
      {vid, lsn, page_range} = key
      page_count = read(f"log/{vid}/{lsn}").page_count
      if not (
          # handle truncate+extend
          page_count.contains(pageidx)
          # segment elimination
          and page_range.contains(pageidx)
          and splinter.contains(pageidx)):
        continue
      page = read(f"pages/{key.sid}/{pageidx}")
      if page:
        return page
      return remote_read_page(vid, key.sid, splinter, pageidx)

def iter_segments(path):
  result = []
  for (vid, scan) in path:
    top = f"segments/{vid}/{scan.start}"
    bottom = f"segments/{vid}/{scan.end}"
    result = chain(result, iter(top..=bottom))
  return result

def remote_read_page(vid, sid, splinter, pageidx):
  segment_size = splinter.cardinality()
  chunk = calculate_chunk_request(sid, splinter, pageidx)
  # fetch the chunk from object storage, loading the pages into `pages/{sid}/{pageidx}`
  return read(f"pages/{sid}/{pageidx}")

def calculate_chunk_request(segment, pageidx):
  # determine which portion of the segment to retrieve by inspecting which portions we've already cached and configuration
  # in general we want to read more than 1 page at a time from s3
  pass
```

#### Volume Writer

```python
class VolumeWriter:
  def new(snapshot):
    self.reader = VolumeReader::new(snapshot)
    self.page_count = snapshot.page_count
    self.sid = SegmentId::random()
    self.splinter = Splinter::new()

  def read(pageidx):
    if self.splinter.contains(pageidx):
      return read(f"pages/{self.sid}/{pageidx}")
    else:
      self.reader.read_page(pageidx)

  def write(pageidx, page):
    self.splinter.insert(pageidx)
    self.page_count = max(self.page_count, pageidx.pages())
    write(f"pages/{self.sid}/{pageidx}", page)

  def truncate(page_count):
    self.page_count = page_count
    delete_range(f"pages/{self.sid}/{page_count}"..)

  # also triggered on drop
  def rollback():
    delete_prefix(f"pages/{sid}")

  def commit():
    snapshot = self.reader.snapshot
    vid = snapshot.vid

    tx = begin()

    # verify we are the latest snapshot
    latest_snapshot = tx.first(f"log/{vid}")
    if snapshot != latest_snapshot:
      raise "concurrent write"

    commit_lsn = snapshot.lsn.next()

    tx.write(f"log/{vid}/{commit_lsn}", Snapshot {
      vid,
      lsn = commit_lsn,
      page_count = self.page_count
    })

    (min, max) = self.splinter.bounds()
    tx.write(
      f"segments/{vid}/{commit_lsn}/{min}/{max}{self.sid}",
      self.splinter
    )

    tx.commit()
```

## TODO: Braindump
I'm considering using Fjall txns to eliminate some/all of my explicit locks...
however, it might be tricky due to how Fjall SSI txns work.
main issue in mind is conflicts between GC and commit due to commit holding a read reference on the entire prefix in order to acquire the latest snapshot.

If instead we continue with locks, we effectively need three locks:
1. to ensure that volume handles aren't corrupted
2. to prevent concurrent volume operations
3. to serialize volume commits

The interesting case seems to be operations. Effectively we want to parallellize operations like push, pull, reset, checkpoint, gc. Most of the operations can be made idempotent (other than where they touch the latest commit and/or the handle). Thus, the main reason for any kind of lock is more about eliminating duplicate work.

Currently the only source of dup work is during pull while recursively walking and backfilling parents. If two volumes share the same parent we may concurrently backfill the parent, potentially leading to wasted work.

Can we handle this case with a object_store fetcher? Sorta like a transaction. Effectively we open a object store reader which lets us fetch keys. Under the hood it dedups concurrent fetches, and also keeps around any fetches until all concurrent txns close.

I think this would work, so long as the code basically worked like:
- open object store reader
- *then* check to see what we need to fetch
- then fetch
- then commit any changes
- then close the reader

Thus, the lifetime of the reader is > the lifetime of visible effect. I think that with this design, there shouldn't be a way for a duplicate fetch to occur which would have been prevented by checking local state.

This utility in theory allows us to eliminate the volume lock and just lean into making more of our operations idempotent.

#### Push Volume

```python
def push_volume(handle_name):
  handle = read(f"handles/{handle_name}")
  (local_lsn, commit) = prepare_commit(handle)
  if remote_commit(commit):
    push_success(handle, commit, local_lsn)
  else:
    push_failure(handle, commit)

def prepare_commit(handle):
  # trigger recovery
  if handle.pending_commit:
    raise InterruptedPush

  { local, remote } = handle

  remote_snapshot = first(f"log/{remote.vid}")
  local_snapshot = first(f"log/{local.vid}")

  top_lsn = local_snapshot.lsn
  bottom_lsn = local.lsn
  sync_range = top_lsn..bottom_lsn

  if sync_range.is_empty():
    raise NothingToCommit

  # build and push segments from commits
  (segment_refs, page_count, commit_hash) = build_and_push_segments(local.vid, lsn_range, remote.vid)

  commit_lsn = remote_snapshot.lsn.next()

  # write out the pending commit
  with handle_lock(handle_name):
    # abort the push if the handle changed since we started the push process
    if handle != read(f"handles/{handle_name}"):
      raise RetryPush

    handle.pending_commit = PendingCommit {
      remote_lsn = commit_lsn,
      commit_hash
    }
    write(f"handles/{handle_name}", handle)

  return (local_snapshot.lsn, Commit {
    snapshot: Snapshot {
      vid: remote.vid,
      lsn: commit_lsn,
      page_count
    },
    hash: commit_hash,
    segments: segment_refs
  })

def build_and_push_segments(local_vid, lsn_range, remote_vid):
  # build up a BTreeMap<PageIdx, SegmentId> containing the last SegmentId for each page in all segments in lsn range on the local volume
  # iterate through the BTreeMap in chunks of MAX_SEGMENT_SIZE
  # building splinters and uploading segments as we go to the remote volume's segment store
  #
  # IF we expect to be querying the remote volume anytime soon, we can optionally write out segments to our local page store.

  pass

def remote_commit(commit):
  path = f"{PREFIX}/{commit.snapshot.vid}/log/{commit.snapshot.lsn}"
  match object_store.write_if_not_exists(path, commit) {
    Ok() => true,
    Err(err) => false
  }

def push_success(handle, commit, local_lsn):
  {vid, lsn} = commit.snapshot

  # prepare a fjall transaction
  tx = begin()

  tx.write(f"log/{vid}/{lsn}", commit.snapshot)

  for segment in commit.segments:
    bounds = segment.splinter.bounds()
    path = f"segments/{vid}/{lsn}/{bounds}/{segment.sid}"
    tx.write(path, segment.splinter)

  new_handle = handle.clone()
  new_handle.pending_commit = None
  new_handle.local = VolumeRef {
    vid: handle.local.vid,
    lsn: local_lsn
  }
  new_handle.remote = VolumeRef {
    vid: handle.remote.vid,
    lsn
  }
  tx.write(f"handles/{handle.name}", new_handle)

  with handle_lock(handle.name):
    # panic if handle has changed
    assert(handle == read(f"handles/{handle_name}"))

    with volume_lock(commit.snapshot.vid):
      # panic if someone already wrote the remote lsn
      assert(not read(f"log/{vid}/{lsn}"))

      tx.commit()

def push_failure(handle, commit):
  # push failed, clear pending commit
  with handle_lock(handle.name):
    # panic if handle has changed
    assert(handle == read(f"handles/{handle_name}"))
    handle.pending_commit = None
    write(f"handles/{handle.name}", handle)
```

#### Pull volume

```python
def fetch_visibility_path(vid, lsn):
  cursor = VolumeRef { vid, lsn }
  path = []
  while cursor:
    { vid, lsn } = cursor

    # load the control if it doesn't exist
    if not tx.read(f"volumes/{vid}/control"):
      control = object_store.fetch(f"{PREFIX}/{vid}/control")
      tx.write(f"volumes/{vid}/control", control)

    # update checkpoints if it doesn't exist or has changed
    etag = tx.read(f"volumes/{vid}/checkpoints").etag
    checkpoints = object_store.fetch(f"{PREFIX}/{vid}/checkpoints", etag)
    if checkpoints:
      tx.write(f"volumes/{vid}/checkpoints", checkpoints.into())

    if checkpoints = tx.read(f"volumes/{vid}/checkpoints"):
      if checkpoint_lsn = checkpoints.for(lsn):
        # found checkpoint, we can terminate the path here
        path.push((vid, (lsn)..=(checkpoint_lsn)))
        return path

    # no checkpoint, so scan to the beginning
    path.push((vid, (lsn)..=1))
    # and iterate to the parent
    cursor = tx.read(f"volumes/{vid}/control").parent

  return (tx, path)

def pull_volume(vid, max_lsn=LSN::MAX):
  # retrieve the latest snapshot <= max_lsn
  snapshot = first(f"log/{vid}/{max_lsn}"..)
  # refresh the visibility path
  path = fetch_visibility_path(vid, snapshot.lsn)

  # fetch all commits in path
  for (vid, scan) in path:
    fetch_commits(vid, scan)


  tx = begin()

  # load the control if it doesn't exist
  if not read(f"volumes/{vid}/control"):
    control = object_store.fetch(f"{PREFIX}/{vid}/control")
    tx.write(f"volumes/{vid}/control", control)

  # update checkpoints if it doesn't exist or has changed
  etag = read(f"volumes/{vid}/checkpoints").etag
  checkpoints = object_store.fetch(f"{PREFIX}/{vid}/checkpoints", etag)
  if checkpoints:
    tx.write(f"volumes/{vid}/checkpoints", checkpoints)

  # start iterating commits either at the most recent checkpoint or after the latest snapshot, whichever is latest
  min_lsn = checkpoints.for(max_lsn).max(snapshot.lsn)

  # pull changes from object store
  for commit in remote_iter_commits(vid, min_lsn..=max_lsn):
    lsn = commit.snapshot.lsn
    tx.write(f"log/{vid}/{lsn}", commit.snapshot)

    for segment in commit.segments:
      bounds = segment.splinter.bounds()
      path = f"segments/{vid}/{lsn}/{bounds}/{segment.sid}"
      tx.write(path, segment.splinter)

  with volume_lock(vid):
    # assert the volume hasn't changed
    assert(snapshot == first(f"log/{vid}"))
    tx.commit()

# lsn_range may have an unbounded end in which case this function will stop fetching commits once it finds the last commit
def fetch_commits(vid, lsn_range):
  for commit in fetch_range(f"{PREFIX}/{vid}/log/{lsn_range}"):
    lsn = commit.snapshot.lsn
    write(f"log/{vid}/{lsn}", commit.snapshot)

    for segment in commit.segments:
      bounds = segment.splinter.bounds()
      path = f"segments/{vid}/{lsn}/{bounds}/{segment.sid}"
      write(path, segment.splinter)
```

#### Sync remote volume to local

In this new architecture we split up pulling the remote volume and syncing it into the local volume.

```python

def sync_remote_to_local(handle_name):
  handle = read(f"handles/{handle_name}")

  # we can safely sync the latest remote snapshot into the local volume if the local volume has no outstanding local changes.

  local_snapshot = first(f"log/{handle.local.vid}")
  if local_snapshot.lsn != handle.local.lsn:
    raise OutstandingLocalChanges

  # no outstanding changes! we can sync
  tx = begin()

  remote_snapshot


```

#### Reset volume

- open new writer vid
- update writer pointer on upstream vid
- eventually GC old vid once read snapshots drop

### Splinter improvements

This RFC depends on the following new [Splinter] features:

- `offset_of(key: u32) -> Option<usize>`
  - returns the zero-based offset of a given key in the Splinter.
  - returns None if the key is not contained by the Splinter.
- `bounds() -> Option<(u32, u32)>`
  - if the Splinter is non-empty, returns the minimum and maximum key stored in the Splinter.
- `iter_range(offsets) -> impl Iterator<Item=u32>`
  - returns an iterator of the keys selected by the offset range.
  - we could instead just override `Iterator::nth` on the existing Splinter iterator. This composes with the rest of the Iterator ecosystem.

All new methods should be implemented on both Splinter and SplinterRef.

## Drawbacks

Why should we _not_ do this?

## Rationale and alternatives

- Why is this design the best in the space of possible designs?
- What other designs have been considered and what is the rationale for not choosing them?
- What is the impact of not doing this?

## Prior art

Discuss prior art, both the good and the bad, in relation to this proposal. A few examples of what this can include are:

- Does this feature exist in other projects and what experience has their community had?
- Papers: Are there any published papers or great posts that discuss this? If you have some relevant papers to refer to, this can serve as a more detailed theoretical background.

This section is intended to encourage you as an author to think about the lessons from other projects, provide readers of your RFC with a fuller picture. If there is no prior art, that is fine - your ideas are interesting to us whether they are brand new or inspired.

## Unresolved questions

- What parts of the design do you expect to resolve through the RFC process before this gets merged?
- What parts of the design do you expect to resolve through the implementation of this feature before stabilization?
- What related issues do you consider out of scope for this RFC that could be addressed in the future independently of the solution that comes out of this RFC?

## Future possibilities

Think about what the natural extension and evolution of your proposal would be and how it would affect the project as a whole in a holistic way. Try to use this section as a tool to more fully consider all possible interactions with the project in your proposal. Also consider how this all fits into the roadmap for the project.

This is also a good place to "dump ideas", if they are out of scope for the RFC you are writing but otherwise related.

If you have tried and cannot think of any future possibilities, you may simply state that you cannot think of anything.

Note that having something written down in the future-possibilities section is not a reason to accept the current or a future RFC; such notes should be in the section on motivation or rationale in this or subsequent RFCs. The section merely provides additional information.

[Fjall]: https://github.com/fjall-rs/fjall/
[`GID`]: https://graft.rs/docs/internals/gid/
[blake3]: https://github.com/BLAKE3-team/BLAKE3
[Splinter]: https://github.com/orbitinghail/splinter-rs
